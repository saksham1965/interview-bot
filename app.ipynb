{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnN+mMmJv62hdsq7HLW58L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saksham1965/interview-bot/blob/main/app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmnBrxojq0xz",
        "outputId": "ebd93fc4-55c0-4199-cbeb-aa5f0702ba08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--start] [--candidate CANDIDATE]\n",
            "                                [--role {software_engineer,data_scientist,product_manager}]\n",
            "                                [--skills SKILLS] [--questions QUESTIONS]\n",
            "                                [--auto-eval] [--llm]\n",
            "\n",
            "AI Interview Bot - Mock Interview CLI\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --start               Start an interactive mock interview\n",
            "  --candidate CANDIDATE\n",
            "                        Candidate name\n",
            "  --role {software_engineer,data_scientist,product_manager}\n",
            "                        Role to interview for\n",
            "  --skills SKILLS       Comma-separated skills (e.g. backend,algorithms)\n",
            "  --questions QUESTIONS\n",
            "                        Number of questions to ask\n",
            "  --auto-eval           Attempt automated LLM evaluation (requires OpenAI key)\n",
            "  --llm                 Allow LLM for question generation and evaluation\n",
            "                        (requires OpenAI key)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import argparse\n",
        "import datetime\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "# Optional import for LLM features. The user may run without OpenAI installed.\n",
        "try:\n",
        "    import openai\n",
        "    OPENAI_AVAILABLE = True\n",
        "except Exception:\n",
        "    OPENAI_AVAILABLE = False\n",
        "\n",
        "# ---------- Config / Templates ----------\n",
        "QUESTION_TEMPLATES = {\n",
        "    \"software_engineer\": {\n",
        "        \"backend\": [\n",
        "            \"Explain how you would design a scalable REST API for a social media feed.\",\n",
        "            \"Describe how you'd debug a memory leak in a Python service.\",\n",
        "            \"Walk me through how you'd design a database schema for an online store's orders.\"\n",
        "        ],\n",
        "        \"algorithms\": [\n",
        "            \"What's your approach to solving an array partitioning problem? Give step-by-step reasoning.\",\n",
        "            \"Describe how a hash table works and its typical use-cases.\",\n",
        "            \"Explain dynamic programming with an example.\"\n",
        "        ]\n",
        "    },\n",
        "    \"data_scientist\": {\n",
        "        \"ml\": [\n",
        "            \"How do you decide whether to use a linear model vs a tree-based model?\",\n",
        "            \"Explain cross-validation and why it's important.\",\n",
        "            \"Describe a time you improved model performance by feature engineering.\"\n",
        "        ],\n",
        "        \"statistics\": [\n",
        "            \"Explain p-values and why they can be misinterpreted.\",\n",
        "            \"When would you use a Bayesian approach vs frequentist?\",\n",
        "            \"Describe how to handle imbalanced classes in classification.\"\n",
        "        ]\n",
        "    },\n",
        "    \"product_manager\": {\n",
        "        \"strategy\": [\n",
        "            \"How would you prioritize features for a new mobile app?\",\n",
        "            \"Describe metrics you would track for a launch and why.\",\n",
        "            \"Tell me about a product tradeoff you made and the outcome.\"\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "DEFAULT_RUBRIC = {\n",
        "    \"clarity\": {\n",
        "        \"description\": \"How clear and structured the answer is.\",\n",
        "        \"weights\": {\"excellent\": 5, \"good\": 3, \"fair\": 1, \"poor\": 0}\n",
        "    },\n",
        "    \"technical_accuracy\": {\n",
        "        \"description\": \"Correctness and depth of technical content.\",\n",
        "        \"weights\": {\"excellent\": 5, \"good\": 3, \"fair\": 1, \"poor\": 0}\n",
        "    },\n",
        "    \"problem_solving\": {\n",
        "        \"description\": \"Approach to solving problems, tradeoffs, and reasoning.\",\n",
        "        \"weights\": {\"excellent\": 5, \"good\": 3, \"fair\": 1, \"poor\": 0}\n",
        "    },\n",
        "    \"communication\": {\n",
        "        \"description\": \"Ability to communicate technical ideas to different audiences.\",\n",
        "        \"weights\": {\"excellent\": 5, \"good\": 3, \"fair\": 1, \"poor\": 0}\n",
        "    }\n",
        "}\n",
        "\n",
        "SESSIONS_DIR = \"interview_sessions\"\n",
        "RUBRICS_FILE = \"rubrics.json\"\n",
        "\n",
        "# ---------- Utilities ----------\n",
        "\n",
        "def ensure_dirs():\n",
        "    os.makedirs(SESSIONS_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def load_or_create_rubrics():\n",
        "    if not os.path.exists(RUBRICS_FILE):\n",
        "        with open(RUBRICS_FILE, \"w\") as f:\n",
        "            json.dump({\"default\": DEFAULT_RUBRIC}, f, indent=2)\n",
        "        return {\"default\": DEFAULT_RUBRIC}\n",
        "    else:\n",
        "        with open(RUBRICS_FILE, \"r\") as f:\n",
        "            return json.load(f)\n",
        "\n",
        "\n",
        "def save_session(session: Dict[str, Any]):\n",
        "    ensure_dirs()\n",
        "    ts = datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
        "    fname = f\"session_{session['candidate_name']}_{ts}.json\"\n",
        "    path = os.path.join(SESSIONS_DIR, fname)\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(session, f, indent=2)\n",
        "    return path\n",
        "\n",
        "# ---------- Question Generation ----------\n",
        "class QuestionGenerator:\n",
        "    def __init__(self, llm_enabled: bool = False):\n",
        "        self.llm_enabled = llm_enabled and OPENAI_AVAILABLE and os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    def sample_questions(self, role: str, skills: List[str], n: int = 5) -> List[str]:\n",
        "        templates = QUESTION_TEMPLATES.get(role, {})\n",
        "        pool = []\n",
        "        for s in skills:\n",
        "            pool.extend(templates.get(s, []))\n",
        "        if not pool:\n",
        "            # fallback: combine generic prompts\n",
        "            pool = [\n",
        "                \"Tell me about a challenging problem you solved recently.\",\n",
        "                \"How do you stay up-to-date in your field?\",\n",
        "                \"Explain a technical decision you made and why.\"\n",
        "            ]\n",
        "\n",
        "        questions = random.sample(pool, min(n, len(pool)))\n",
        "\n",
        "        # Optionally augment with LLM (if enabled)\n",
        "        if self.llm_enabled:\n",
        "            try:\n",
        "                augmented = self._augment_with_llm(role, skills, questions)\n",
        "                if augmented:\n",
        "                    questions = augmented\n",
        "            except Exception:\n",
        "                pass  # keep the sampled ones if LLM fails\n",
        "\n",
        "        return questions\n",
        "\n",
        "    def _augment_with_llm(self, role: str, skills: List[str], base_questions: List[str]) -> Optional[List[str]]:\n",
        "        # Example: call OpenAI to rewrite/expand questions to be more varied\n",
        "        prompt = (\n",
        "            f\"You are a helpful interview question generator. Role: {role}. Skills: {', '.join(skills)}.\\n\"\n",
        "            f\"Given these base prompts: {base_questions}\\n\"\n",
        "            \"Return 1-to-1 rewritten versions that are clear, distinct, and targeted. Return as JSON array.\"\n",
        "        )\n",
        "        resp = openai.Completion.create(\n",
        "            engine=\"text-davinci-003\",\n",
        "            prompt=prompt,\n",
        "            max_tokens=500,\n",
        "            temperature=0.8,\n",
        "            n=1\n",
        "        )\n",
        "        text = resp.choices[0].text.strip()\n",
        "        try:\n",
        "            out = json.loads(text)\n",
        "            if isinstance(out, list):\n",
        "                return out\n",
        "        except Exception:\n",
        "            return None\n",
        "        return None\n",
        "\n",
        "# ---------- Evaluator / Rubric ----------\n",
        "class Evaluator:\n",
        "    def __init__(self, rubric: Dict[str, Any], llm_enabled: bool = False):\n",
        "        self.rubric = rubric\n",
        "        self.llm_enabled = llm_enabled and OPENAI_AVAILABLE and os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    def evaluate_manual(self, answer: str, scores: Dict[str, str]) -> Dict[str, Any]:\n",
        "        \"\"\"Use manual rubric ratings (excellent/good/fair/poor) provided in scores dict.\"\"\"\n",
        "        feedback = {}\n",
        "        total = 0\n",
        "        max_total = 0\n",
        "        for criterion, cfg in self.rubric.items():\n",
        "            weight_map = cfg.get(\"weights\", {})\n",
        "            rating = scores.get(criterion, \"fair\")\n",
        "            pts = weight_map.get(rating, 0)\n",
        "            max_pts = max(weight_map.values()) if weight_map else 1\n",
        "            feedback[criterion] = {\n",
        "                \"rating\": rating,\n",
        "                \"points\": pts,\n",
        "                \"description\": cfg.get(\"description\", \"\")\n",
        "            }\n",
        "            total += pts\n",
        "            max_total += max_pts\n",
        "        normalized_score = round((total / (max_total or 1)) * 100, 1)\n",
        "        return {\"feedback\": feedback, \"score_percent\": normalized_score}\n",
        "\n",
        "    def evaluate_auto(self, question: str, answer: str) -> Dict[str, Any]:\n",
        "        \"\"\"Use an LLM to automatically rate the answer per rubric. Falls back if LLM not available.\"\"\"\n",
        "        if not self.llm_enabled:\n",
        "            return {\"error\": \"LLM not enabled or available\"}\n",
        "\n",
        "        # Build instruction\n",
        "        rubric_text = json.dumps(self.rubric)\n",
        "        prompt = (\n",
        "            f\"Evaluate the candidate's answer for the following question. Return a JSON object with each rubric criterion mapped to one of [excellent, good, fair, poor].\\n\"\n",
        "            f\"Question: {question}\\nAnswer: {answer}\\nRubric: {rubric_text}\\nReturn only valid JSON.\"\n",
        "        )\n",
        "        resp = openai.Completion.create(\n",
        "            engine=\"text-davinci-003\",\n",
        "            prompt=prompt,\n",
        "            max_tokens=400,\n",
        "            temperature=0.0,\n",
        "            n=1\n",
        "        )\n",
        "        text = resp.choices[0].text.strip()\n",
        "        try:\n",
        "            out = json.loads(text)\n",
        "            # convert to manual evaluate form\n",
        "            return self.evaluate_manual(answer, out)\n",
        "        except Exception:\n",
        "            return {\"error\": \"Failed to parse LLM output\", \"raw\": text}\n",
        "\n",
        "# ---------- Interview Session Flow ----------\n",
        "class InterviewSession:\n",
        "    def __init__(self, candidate_name: str, role: str, skills: List[str], rubric_name: str = \"default\", llm_enabled: bool = False):\n",
        "        self.candidate_name = candidate_name\n",
        "        self.role = role\n",
        "        self.skills = skills\n",
        "        self.started_at = datetime.datetime.utcnow().isoformat() + \"Z\"\n",
        "        self.questions: List[Dict[str, Any]] = []\n",
        "        self.rubrics = load_or_create_rubrics()\n",
        "        self.rubric = self.rubrics.get(rubric_name, self.rubrics.get(\"default\", DEFAULT_RUBRIC))\n",
        "        self.generator = QuestionGenerator(llm_enabled=llm_enabled)\n",
        "        self.evaluator = Evaluator(self.rubric, llm_enabled=llm_enabled)\n",
        "\n",
        "    def prepare_questions(self, n: int = 5):\n",
        "        qs = self.generator.sample_questions(self.role, self.skills, n=n)\n",
        "        self.questions = [{\"q\": q, \"answer\": None, \"eval\": None} for q in qs]\n",
        "\n",
        "    def run_cli(self, auto_eval: bool = False):\n",
        "        print(f\"Starting mock interview for {self.candidate_name} — role: {self.role}\")\n",
        "        for i, item in enumerate(self.questions, 1):\n",
        "            print(f\"\\nQ{i}: {item['q']}\")\n",
        "            ans = input(\"Your answer (type and press enter):\\n\")\n",
        "            item['answer'] = ans\n",
        "            if auto_eval and self.evaluator.llm_enabled:\n",
        "                ev = self.evaluator.evaluate_auto(item['q'], ans)\n",
        "                item['eval'] = ev\n",
        "                print(\"Auto-evaluation:\")\n",
        "                print(json.dumps(ev, indent=2))\n",
        "            else:\n",
        "                # allow manual scoring\n",
        "                scores = {}\n",
        "                print(\"Rate the following criteria: excellent / good / fair / poor (press enter to use 'fair')\")\n",
        "                for crit in self.rubric.keys():\n",
        "                    r = input(f\" - {crit}: \") or \"fair\"\n",
        "                    scores[crit] = r\n",
        "                ev = self.evaluator.evaluate_manual(ans, scores)\n",
        "                item['eval'] = ev\n",
        "                print(\"Evaluation result:\")\n",
        "                print(json.dumps(ev, indent=2))\n",
        "\n",
        "    def summary(self) -> Dict[str, Any]:\n",
        "        # Aggregate\n",
        "        items = []\n",
        "        scores = []\n",
        "        for item in self.questions:\n",
        "            items.append({\"question\": item['q'], \"answer\": item['answer'], \"evaluation\": item['eval']})\n",
        "            if item['eval'] and 'score_percent' in item['eval']:\n",
        "                scores.append(item['eval']['score_percent'])\n",
        "        avg_score = round(sum(scores) / len(scores), 1) if scores else None\n",
        "        summary = {\n",
        "            \"candidate\": self.candidate_name,\n",
        "            \"role\": self.role,\n",
        "            \"started_at\": self.started_at,\n",
        "            \"average_score\": avg_score,\n",
        "            \"questions\": items\n",
        "        }\n",
        "        return summary\n",
        "\n",
        "# ---------- CLI Entrypoint ----------\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"AI Interview Bot - Mock Interview CLI\")\n",
        "    parser.add_argument(\"--start\", action=\"store_true\", help=\"Start an interactive mock interview\")\n",
        "    parser.add_argument(\"--candidate\", type=str, help=\"Candidate name\")\n",
        "    parser.add_argument(\"--role\", type=str, choices=list(QUESTION_TEMPLATES.keys()), help=\"Role to interview for\")\n",
        "    parser.add_argument(\"--skills\", type=str, help=\"Comma-separated skills (e.g. backend,algorithms)\")\n",
        "    parser.add_argument(\"--questions\", type=int, default=5, help=\"Number of questions to ask\")\n",
        "    parser.add_argument(\"--auto-eval\", action=\"store_true\", help=\"Attempt automated LLM evaluation (requires OpenAI key)\")\n",
        "    parser.add_argument(\"--llm\", action=\"store_true\", help=\"Allow LLM for question generation and evaluation (requires OpenAI key)\")\n",
        "\n",
        "    # Use parse_known_args to ignore arguments passed by Jupyter/Colab kernel\n",
        "    # and only process arguments relevant to the script.\n",
        "    args = parser.parse_known_args()[0]\n",
        "\n",
        "    if args.start:\n",
        "        candidate = args.candidate or input(\"Candidate name: \")\n",
        "        role = args.role or input(f\"Role ({', '.join(QUESTION_TEMPLATES.keys())}): \")\n",
        "        skills_str = args.skills or input(\"Skills (comma separated): \")\n",
        "        skills = [s.strip() for s in skills_str.split(\",\") if s.strip()]\n",
        "        session = InterviewSession(candidate, role, skills, llm_enabled=args.llm)\n",
        "        session.prepare_questions(n=args.questions)\n",
        "        session.run_cli(auto_eval=args.auto_eval)\n",
        "        summ = session.summary()\n",
        "        path = save_session(summ)\n",
        "        print(f\"\\nSession saved to: {path}\")\n",
        "        print(\"Summary:\")\n",
        "        print(json.dumps(summ, indent=2))\n",
        "    else:\n",
        "        parser.print_help()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf6ff405",
        "outputId": "88cf204c-e4fc-4fea-c2eb-7d06b441266a"
      },
      "source": [
        "import sys\n",
        "\n",
        "# Backup original sys.argv\n",
        "original_argv = sys.argv\n",
        "\n",
        "# Simulate command line arguments\n",
        "sys.argv = [\n",
        "    'colab_kernel_launcher.py',\n",
        "    '--start',\n",
        "    '--candidate', 'Alice Smith',\n",
        "    '--role', 'software_engineer',\n",
        "    '--skills', 'backend,algorithms',\n",
        "    '--questions', '2' # Limiting to 2 questions for demonstration\n",
        "]\n",
        "\n",
        "# Call the main function\n",
        "try:\n",
        "    main()\n",
        "finally:\n",
        "    # Restore original sys.argv\n",
        "    sys.argv = original_argv\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2555897659.py:209: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  self.started_at = datetime.datetime.utcnow().isoformat() + \"Z\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting mock interview for Alice Smith — role: software_engineer\n",
            "\n",
            "Q1: Explain how you would design a scalable REST API for a social media feed.\n"
          ]
        }
      ]
    }
  ]
}